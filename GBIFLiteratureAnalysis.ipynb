{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pypandoc\n",
    "import pycountry\n",
    "import re\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert country codes to country names\n",
    "def get_country_name(code):\n",
    "    try:\n",
    "        return pycountry.countries.get(alpha_2=code).name\n",
    "    except AttributeError:\n",
    "        return code  # Return the code if it can't find the name\n",
    "\n",
    "# Function to convert country codes to alpha-3 format\n",
    "def convert_country_code_to_alpha3(code):\n",
    "    try:\n",
    "        return pycountry.countries.get(alpha_2=code).alpha_3\n",
    "    except AttributeError:\n",
    "        return code  # Return the code if it can't find the alpha-3 code\n",
    "\n",
    "# Function to fetch taxon name from GBIF API using taxon key\n",
    "def fetch_taxon_name(taxon_key):\n",
    "    url = f\"https://api.gbif.org/v1/species/{taxon_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get('scientificName', '')\n",
    "    return ''\n",
    "\n",
    "# Function to preprocess Markdown content\n",
    "def preprocess_markdown(content):\n",
    "    # Replace problematic Unicode characters with ASCII equivalents or remove them\n",
    "    replacements = {\n",
    "        '\\u223C': '~',  # Replace ∼ with ~\n",
    "        '\\u2010': '-',  # Replace ‐ with -\n",
    "        '\\u2011': '-',  # Replace ‑ with -\n",
    "        '\\u2012': '-',  # Replace ‒ with -\n",
    "        '\\u2013': '-',  # Replace – with -\n",
    "        '\\u2014': '-',  # Replace — with -\n",
    "        '\\u2015': '-',  # Replace ― with -\n",
    "        '\\u2018': \"'\",  # Replace ‘ with '\n",
    "        '\\u2019': \"'\",  # Replace ’ with '\n",
    "        '\\u201C': '\"',  # Replace “ with \"\n",
    "        '\\u201D': '\"',  # Replace ” with \"\n",
    "        '\\u2026': '...',  # Replace … with ...\n",
    "        '\\u2212': '-',  # Replace − with -\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        content = content.replace(old, new)\n",
    "    \n",
    "    # Remove any remaining non-ASCII characters\n",
    "    content = re.sub(r'[^\\x00-\\x7F]+', '', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Read the CSV files and extract unique DOIs\n",
    "file_path = 'KYMS.csv'  # Update this with your file path\n",
    "download_summary_path = 'download_summary.csv'  # Path to download_summary.csv\n",
    "\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "download_summary_df = pd.read_csv(download_summary_path, encoding='utf-8')\n",
    "\n",
    "# Calculate the number of GBIF IDs associated with each DOI\n",
    "gbif_id_counts = df.groupby('doi')['gbifID'].nunique().to_dict()\n",
    "\n",
    "# Create a dictionary to map gbifDownloadKey to total_records\n",
    "download_summary_dict = download_summary_df.set_index('key')['total_records'].to_dict()\n",
    "\n",
    "dois = df['doi'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Fetch metadata using GBIF API with progress bar\n",
    "def fetch_metadata_gbif(doi_list):\n",
    "    metadata_list = []\n",
    "    for doi in tqdm(doi_list, desc=\"Fetching metadata\"):\n",
    "        url = f\"https://api.gbif.org/v1/literature/search?doi={doi}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            metadata_list.append(response.json())\n",
    "        else:\n",
    "            print(f\"Failed to fetch metadata for DOI: {doi}\")\n",
    "        time.sleep(1)  # To respect API rate limits\n",
    "    return metadata_list\n",
    "\n",
    "metadata_list = fetch_metadata_gbif(dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b922b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Extract relevant fields from metadata and fetch taxon names\n",
    "def extract_relevant_fields(metadata):\n",
    "    if not metadata or 'results' not in metadata or not metadata['results']:\n",
    "        return None\n",
    "    \n",
    "    result = metadata['results'][0]\n",
    "    \n",
    "    try:\n",
    "        authors = \", \".join([f\"{author['firstName']} {author['lastName']}\" for author in result.get('authors', [])])\n",
    "    except KeyError:\n",
    "        authors = ''\n",
    "\n",
    "    title = result.get('title', '')\n",
    "    abstract = result.get('abstract', '')\n",
    "    doi = result.get('identifiers', {}).get('doi', '')\n",
    "    doi_link = f\"https://doi.org/{doi}\"\n",
    "    countries_of_coverage = \", \".join(result.get('countriesOfCoverage', []))\n",
    "    countries_of_researcher = \", \".join([get_country_name(code) for code in result.get('countriesOfResearcher', [])])\n",
    "    countries_of_researcher_codes = result.get('countriesOfResearcher', [])\n",
    "    published = result.get('published', '')\n",
    "    \n",
    "    gbif_taxon_key = result.get('gbifTaxonKey', [])\n",
    "    gbif_higher_taxon_key = result.get('gbifHigherTaxonKey', [])\n",
    "    \n",
    "    gbif_taxon_names = [fetch_taxon_name(key) for key in gbif_taxon_key]\n",
    "    gbif_higher_taxon_names = [fetch_taxon_name(key) for key in gbif_higher_taxon_key]\n",
    "    \n",
    "    citation_type = result.get('citationType', '')\n",
    "    literature_type = result.get('literatureType', '')\n",
    "    open_access = result.get('openAccess', '')\n",
    "    peer_review = result.get('peerReview', '')\n",
    "    publisher = result.get('publisher', '')\n",
    "    relevance = \", \".join(result.get('relevance', []))\n",
    "    source = result.get('source', '')\n",
    "    language = result.get('language', '')\n",
    "    year = result.get('year', '')\n",
    "    \n",
    "    topics = \", \".join(result.get('topics', []))\n",
    "    \n",
    "    # Get the number of GBIF IDs associated with this DOI\n",
    "    num_gbif_ids = gbif_id_counts.get(doi, 0)\n",
    "    \n",
    "    # Get the total records for gbifDownloadKey\n",
    "    gbif_download_keys = result.get('gbifDownloadKey', [])\n",
    "    total_records = sum(download_summary_dict.get(key, 0) for key in gbif_download_keys)\n",
    "    \n",
    "    return {\n",
    "        'authors': authors,\n",
    "        'title': title,\n",
    "        'abstract': abstract,\n",
    "        'doi': doi_link,\n",
    "        'numGbifIds': num_gbif_ids,\n",
    "        'totalRecords': total_records,\n",
    "        'countriesOfCoverage': countries_of_coverage,\n",
    "        'countriesOfResearcher': countries_of_researcher,\n",
    "        'countriesOfResearcherCodes': countries_of_researcher_codes,\n",
    "        'published': published,\n",
    "        'gbifTaxonKey': \", \".join(map(str, gbif_taxon_key)),\n",
    "        'gbifHigherTaxonKey': \", \".join(map(str, gbif_higher_taxon_key)),\n",
    "        'gbifTaxonNames': \", \".join(gbif_taxon_names),\n",
    "        'gbifHigherTaxonNames': \", \".join(gbif_higher_taxon_names),\n",
    "        'citationType': citation_type,\n",
    "        'literatureType': literature_type,\n",
    "        'openAccess': open_access,\n",
    "        'peerReview': peer_review,\n",
    "        'publisher': publisher,\n",
    "        'relevance': relevance,\n",
    "        'source': source,\n",
    "        'language': language,\n",
    "        'year': year,\n",
    "        'topics': topics\n",
    "    }\n",
    "\n",
    "relevant_data = [extract_relevant_fields(metadata) for metadata in metadata_list if metadata and 'results' in metadata and metadata['results']]\n",
    "\n",
    "# Convert the extracted data into a DataFrame\n",
    "df_relevant_data = pd.DataFrame(relevant_data)\n",
    "print(df_relevant_data.head())\n",
    "\n",
    "# Save the results to a spreadsheet\n",
    "output_excel_path = 'gbif_literature_data.xlsx'\n",
    "df_relevant_data.to_excel(output_excel_path, index=False)\n",
    "print(f\"Results saved to {output_excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a185c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Extract keywords and topics\n",
    "def extract_keywords_gbif(metadata):\n",
    "    try:\n",
    "        return metadata['results'][0]['keywords']\n",
    "    except (KeyError, IndexError):\n",
    "        return []\n",
    "\n",
    "def extract_topics_gbif(metadata):\n",
    "    try:\n",
    "        return metadata['results'][0]['topics']\n",
    "    except (KeyError, IndexError):\n",
    "        return []\n",
    "\n",
    "keywords_list = [extract_keywords_gbif(metadata) for metadata in metadata_list if metadata and 'results' in metadata and metadata['results']]\n",
    "topics_list = [extract_topics_gbif(metadata) for metadata in metadata_list if metadata and 'results' in metadata and metadata['results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30947546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create the network\n",
    "def create_network(items_list):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for items in items_list:\n",
    "        for i, item1 in enumerate(items):\n",
    "            for item2 in items[i+1:]:\n",
    "                if G.has_edge(item1, item2):\n",
    "                    G[item1][item2]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(item1, item2, weight=1)\n",
    "    \n",
    "    return G\n",
    "\n",
    "keyword_network = create_network(keywords_list)\n",
    "topic_network = create_network(topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e55fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save the networks as GraphML files\n",
    "# Add topic counts as node attributes to the topic network\n",
    "topic_counts = pd.Series([topic for sublist in topics_list for topic in sublist]).value_counts()\n",
    "for node in topic_network.nodes():\n",
    "    topic_network.nodes[node]['count'] = topic_counts[node]\n",
    "\n",
    "nx.write_graphml(keyword_network, 'keyword_network.graphml')\n",
    "nx.write_graphml(topic_network, 'topic_network.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Plot the networks\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Use DejaVu Sans font\n",
    "dejavu_font_path = fm.findfont(fm.FontProperties(family='DejaVu Sans'))\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "# Plot the Keyword Network\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Keyword Network\", fontsize=14)\n",
    "pos = nx.spring_layout(keyword_network, k=0.1)\n",
    "nx.draw(keyword_network, pos, with_labels=True, node_size=50, font_size=10, font_color='blue', edge_color='gray')\n",
    "plt.show()\n",
    "\n",
    "# Update Node Sizes for Topic Network\n",
    "node_sizes = [topic_counts.get(topic, 1) * 10 for topic in topic_network.nodes()]  # Ensure default size if topic not in topic_counts\n",
    "\n",
    "# Plot the Topic Network\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Topic Network\", fontsize=14)\n",
    "pos = nx.spring_layout(topic_network, k=0.1)\n",
    "nx.draw(topic_network, pos, with_labels=True, node_size=node_sizes, font_size=10, font_color='green', edge_color='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create Markdown files for each publication\n",
    "output_dir = 'publication_markdowns'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for index, row in df_relevant_data.iterrows():\n",
    "    md_content = f\"\"\"\n",
    "# {row['title']}\n",
    "\n",
    "**Authors**: {row['authors']}\n",
    "\n",
    "**Abstract**: {row['abstract']}\n",
    "\n",
    "**DOI**: [{row['doi']}]({row['doi']})\n",
    "\n",
    "**Number of GBIF IDs**: {row['numGbifIds']}\n",
    "\n",
    "**Total Records**: {row['totalRecords']}\n",
    "\n",
    "**Countries of Coverage**: {row['countriesOfCoverage']}\n",
    "\n",
    "**Countries of Researcher**: {row['countriesOfResearcher']}\n",
    "\n",
    "**Published**: {row['published']}\n",
    "\n",
    "**GBIF Taxon Key**: {row['gbifTaxonKey']}\n",
    "**GBIF Taxon Names**: {row['gbifTaxonNames']}\n",
    "\n",
    "**GBIF Higher Taxon Key**: {row['gbifHigherTaxonKey']}\n",
    "**GBIF Higher Taxon Names**: {row['gbifHigherTaxonNames']}\n",
    "\n",
    "**Citation Type**: {row['citationType']}\n",
    "**Literature Type**: {row['literatureType']}\n",
    "**Open Access**: {row['openAccess']}\n",
    "**Peer Review**: {row['peerReview']}\n",
    "**Publisher**: {row['publisher']}\n",
    "**Relevance**: {row['relevance']}\n",
    "**Source**: {row['source']}\n",
    "**Language**: {row['language']}\n",
    "**Year**: {row['year']}\n",
    "**Topics**: {row['topics']}\n",
    "\"\"\"\n",
    "    # Preprocess the Markdown content to handle problematic characters\n",
    "    md_content = preprocess_markdown(md_content)\n",
    "\n",
    "    md_file_path = os.path.join(output_dir, f\"publication_{index + 1}.md\")\n",
    "    with open(md_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(md_content)\n",
    "\n",
    "    # Convert Markdown to PDF using the template\n",
    "    pdf_file_path = os.path.join(output_dir, f\"publication_{index + 1}.pdf\")\n",
    "    pypandoc.convert_file(md_file_path, 'pdf', outputfile=pdf_file_path, extra_args=['--template=template.tex'])\n",
    "\n",
    "print(f\"Markdown and PDF files created in the folder: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Create a global choropleth map for countriesOfResearcher\n",
    "# Add a dictionary for country coordinates\n",
    "country_coordinates = {\n",
    "    'AF': (67.709953, 33.93911),\n",
    "    'AL': (20.168331, 41.153332),\n",
    "    'DZ': (1.659626, 28.033886),\n",
    "    'AO': (17.873887, -11.202692),\n",
    "    'AR': (-63.61667199999999, -38.416097),\n",
    "    'AM': (45.038189, 40.069099),\n",
    "    'AU': (133.775136, -25.274398),\n",
    "    'AT': (14.550072, 47.516231),\n",
    "    'AZ': (47.576927, 40.143105),\n",
    "    'BS': (-77.39627999999999, 25.03428),\n",
    "    'BH': (50.637772, 25.930414),\n",
    "    'BD': (90.356331, 23.684994),\n",
    "    'BY': (27.953389, 53.709807),\n",
    "    'BE': (4.469936, 50.503887),\n",
    "    'BZ': (-88.49765, 17.189877),\n",
    "    'BJ': (2.315834, 9.30769),\n",
    "    'BT': (90.433601, 27.514162),\n",
    "    'BO': (-63.58865299999999, -16.290154),\n",
    "    'BA': (17.679076, 43.915886),\n",
    "    'BW': (24.684866, -22.328474),\n",
    "    'BR': (-51.92528, -14.235004),\n",
    "    'BN': (114.727669, 4.535277),\n",
    "    'BG': (25.48583, 42.733883),\n",
    "    'BF': (-1.561593, 12.238333),\n",
    "    'BI': (29.918886, -3.373056),\n",
    "    'KH': (104.990963, 12.565679),\n",
    "    'CM': (12.354722, 7.369721999999999),\n",
    "    'CA': (-106.346771, 56.130366),\n",
    "    'CV': (-24.013197, 16.002082),\n",
    "    'CF': (20.939444, 6.611110999999999),\n",
    "    'TD': (18.732207, 15.454166),\n",
    "    'CL': (-71.542969, -35.675147),\n",
    "    'CN': (104.195397, 35.86166),\n",
    "    'CO': (-74.297333, 4.570868),\n",
    "    'KM': (43.872219, -11.875001),\n",
    "    'CD': (21.758664, -4.038333),\n",
    "    'CG': (15.827659, -0.228021),\n",
    "    'CR': (-83.753428, 9.748916999999999),\n",
    "    'HR': (15.2, 45.1),\n",
    "    'CU': (-77.781167, 21.521757),\n",
    "    'CY': (33.429859, 35.126413),\n",
    "    'CZ': (15.472962, 49.817492),\n",
    "    'DK': (9.501785, 56.26392),\n",
    "    'DJ': (42.590275, 11.825138),\n",
    "    'DM': (-61.370976, 15.414999),\n",
    "    'DO': (-70.162651, 18.735693),\n",
    "    'EC': (-78.18340599999999, -1.831239),\n",
    "    'EG': (30.802498, 26.820553),\n",
    "    'SV': (-88.89653, 13.794185),\n",
    "    'GQ': (10.267895, 1.650801),\n",
    "    'ER': (39.782334, 15.179384),\n",
    "    'EE': (25.013607, 58.595272),\n",
    "    'SZ': (31.465866, -26.522503),\n",
    "    'ET': (39.782334, 9.145),\n",
    "    'FJ': (178.065032, -17.713371),\n",
    "    'FI': (25.748151, 61.92410999999999),\n",
    "    'FR': (2.213749, 46.603354),\n",
    "    'GA': (11.609444, -0.803689),\n",
    "    'GM': (-15.310139, 13.443182),\n",
    "    'GE': (43.356892, 42.315407),\n",
    "    'DE': (10.451526, 51.165691),\n",
    "    'GH': (-1.023194, 7.946527),\n",
    "    'GR': (21.824312, 39.074208),\n",
    "    'GD': (-61.604171, 12.262776),\n",
    "    'GT': (-90.23075899999999, 15.783471),\n",
    "    'GN': (-9.696645, 9.945587),\n",
    "    'GW': (-15.180413, 11.803749),\n",
    "    'GY': (-58.93018, 4.860416),\n",
    "    'HT': (-72.285215, 18.971187),\n",
    "    'HN': (-86.241905, 15.199999),\n",
    "    'HU': (19.503304, 47.162494),\n",
    "    'IS': (-19.020835, 64.963051),\n",
    "    'IN': (78.96288, 20.593684),\n",
    "    'ID': (113.921327, -0.789275),\n",
    "    'IR': (53.688046, 32.427908),\n",
    "    'IQ': (43.679291, 33.223191),\n",
    "    'IE': (-8.24389, 53.41291),\n",
    "    'IL': (34.851612, 31.046051),\n",
    "    'IT': (12.56738, 41.87194),\n",
    "    'JM': (-77.297508, 18.109581),\n",
    "    'JP': (138.252924, 36.204824),\n",
    "    'JO': (36.238414, 30.585164),\n",
    "    'KZ': (66.923684, 48.019573),\n",
    "    'KE': (37.906193, -0.023559),\n",
    "    'KI': (-168.734039, -3.370417),\n",
    "    'KP': (127.510093, 40.339852),\n",
    "    'KR': (127.766922, 35.907757),\n",
    "    'KW': (47.481766, 29.31166),\n",
    "    'KG': (74.766098, 41.20438),\n",
    "    'LA': (102.495496, 19.85627),\n",
    "    'LV': (24.603189, 56.879635),\n",
    "    'LB': (35.862285, 33.854721),\n",
    "    'LS': (28.233608, -29.609988),\n",
    "    'LR': (-9.429499000000002, 6.428055),\n",
    "    'LY': (17.228331, 26.3351),\n",
    "    'LI': (9.555373, 47.166),\n",
    "    'LT': (23.881275, 55.169438),\n",
    "    'LU': (6.129582999999999, 49.815273),\n",
    "    'MK': (21.745275, 41.608635),\n",
    "    'MG': (46.869107, -18.766947),\n",
    "    'MW': (34.301525, -13.254308),\n",
    "    'MY': (101.975766, 4.210484),\n",
    "    'MV': (73.22068, 3.202778),\n",
    "    'ML': (-3.996166, 17.570692),\n",
    "    'MT': (14.375416, 35.937496),\n",
    "    'MH': (171.184478, 7.131474),\n",
    "    'MR': (-10.940835, 21.00789),\n",
    "    'MU': (57.55215200000001, -20.348404),\n",
    "    'MX': (-102.552784, 23.634501),\n",
    "    'FM': (150.550812, 7.425554),\n",
    "    'MD': (28.369885, 47.411631),\n",
    "    'MC': (7.412841, 43.750298),\n",
    "    'MN': (103.846656, 46.862496),\n",
    "    'ME': (19.37439, 42.708678),\n",
    "    'MA': (-7.092619999999999, 31.791702),\n",
    "    'MZ': (35.529562, -18.665695),\n",
    "    'MM': (95.956223, 21.913965),\n",
    "    'NA': (18.49041, -22.95764),\n",
    "    'NR': (166.931503, -0.522778),\n",
    "    'NP': (84.12400799999999, 28.394857),\n",
    "    'NL': (5.291265999999999, 52.132633),\n",
    "    'NZ': (174.885971, -40.900557),\n",
    "    'NI': (-85.207229, 12.865416),\n",
    "    'NE': (8.081666, 17.607789),\n",
    "    'NG': (8.675277, 9.081999),\n",
    "    'NO': (8.468945999999999, 60.47202399999999),\n",
    "    'OM': (55.923255, 21.512583),\n",
    "    'PK': (69.34511599999999, 30.375321),\n",
    "    'PW': (134.58252, 7.51498),\n",
    "    'PA': (-80.782127, 8.537981),\n",
    "    'PG': (143.95555, -6.314992999999999),\n",
    "    'PY': (-58.443832, -23.442503),\n",
    "    'PE': (-75.015152, -9.189967),\n",
    "    'PH': (121.774017, 12.879721),\n",
    "    'PL': (19.145136, 51.919438),\n",
    "    'PT': (-8.224454, 39.39987199999999),\n",
    "    'QA': (51.183884, 25.354826),\n",
    "    'RO': (24.96676, 45.943161),\n",
    "    'RU': (105.318756, 61.52401),\n",
    "    'RW': (29.873888, -1.940278),\n",
    "    'KN': (-62.782998, 17.357822),\n",
    "    'LC': (-60.978893, 13.909444),\n",
    "    'VC': (-61.287228, 12.984305),\n",
    "    'WS': (-172.104629, -13.759029),\n",
    "    'SM': (12.457777, 43.94236),\n",
    "    'ST': (6.613081, 0.18636),\n",
    "    'SA': (45.079162, 23.885942),\n",
    "    'SN': (-14.452362, 14.497401),\n",
    "    'RS': (21.005859, 44.016521),\n",
    "    'SC': (55.491977, -4.679574),\n",
    "    'SL': (-11.779889, 8.460555),\n",
    "    'SG': (103.819836, 1.352083),\n",
    "    'SK': (19.699024, 48.669026),\n",
    "    'SI': (14.995463, 46.151241),\n",
    "    'SB': (160.156194, -9.64571),\n",
    "    'SO': (46.199616, 5.152149),\n",
    "    'ZA': (22.937506, -30.559482),\n",
    "    'SS': (31.3069788, 6.877),\n",
    "    'ES': (-3.74922, 40.46366700000001),\n",
    "    'LK': (80.77179699999999, 7.873053999999999),\n",
    "    'SD': (30.217636, 12.862807),\n",
    "    'SR': (-56.027783, 3.919305),\n",
    "    'SZ': (31.465866, -26.522503),\n",
    "    'SE': (18.643501, 60.12816100000001),\n",
    "    'CH': (8.227511999999999, 46.818188),\n",
    "    'SY': (38.996815, 34.80207499999999),\n",
    "    'TJ': (71.276093, 38.861034),\n",
    "    'TZ': (34.888822, -6.369028),\n",
    "    'TH': (100.992541, 15.870032),\n",
    "    'TL': (125.727539, -8.874217),\n",
    "    'TG': (0.824782, 8.619543),\n",
    "    'TO': (-175.198242, -21.178986),\n",
    "    'TT': (-61.222503, 10.691803),\n",
    "    'TN': (9.537499, 33.886917),\n",
    "    'TR': (35.243322, 38.963745),\n",
    "    'TM': (59.556278, 38.969719),\n",
    "    'TV': (179.194, -7.109535),\n",
    "    'UG': (32.290275, 1.373333),\n",
    "    'UA': (31.16558, 48.379433),\n",
    "    'AE': (53.847818, 23.424076),\n",
    "    'GB': (-3.435973, 55.378051),\n",
    "    'US': (-95.712891, 37.09024),\n",
    "    'UY': (-55.765835, -32.522779),\n",
    "    'UZ': (64.585262, 41.377491),\n",
    "    'VU': (166.959158, -15.376706),\n",
    "    'VE': (-66.58973, 6.42375),\n",
    "    'VN': (108.277199, 14.058324),\n",
    "    'YE': (48.516388, 15.552727),\n",
    "    'ZM': (27.849332, -13.133897),\n",
    "    'ZW': (29.154857, -19.015438)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de983e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Create a global choropleth map for countriesOfResearcher\n",
    "def create_choropleth_map(df):\n",
    "    # Explode the countriesOfResearcherCodes column to get one row per country code\n",
    "    df_exploded = df.explode('countriesOfResearcherCodes')\n",
    "    # Group by country code and count the occurrences\n",
    "    country_counts = df_exploded['countriesOfResearcherCodes'].value_counts().reset_index()\n",
    "    country_counts.columns = ['country_code_2', 'count']\n",
    "    # Add alpha-3 country codes\n",
    "    country_counts['country_code_3'] = country_counts['country_code_2'].apply(convert_country_code_to_alpha3)\n",
    "    \n",
    "    # Save the country counts to a CSV file\n",
    "    country_counts.to_csv('country_counts.csv', index=False)\n",
    "    \n",
    "    # Debug: Print the country counts\n",
    "    print(country_counts.head())\n",
    "    \n",
    "    # Create the choropleth map using matplotlib\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    m = Basemap(projection='moll', lon_0=0)\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    m.drawmapboundary(fill_color='aqua')\n",
    "    m.fillcontinents(color='coral', lake_color='aqua')\n",
    "\n",
    "    # Add the data\n",
    "    for _, row in country_counts.iterrows():\n",
    "        try:\n",
    "            country_code = row['country_code_2']\n",
    "            count = row['count']\n",
    "            if country_code in country_coordinates:\n",
    "                lon, lat = country_coordinates[country_code]\n",
    "                x, y = m(lon, lat)\n",
    "                m.plot(x, y, 'bo', markersize=count/2)  # Adjusted marker size for better visibility\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot {country_code}: {e}\")\n",
    "\n",
    "    plt.title(\"Number of Publications by Country of Researcher (Mollweide Projection)\")\n",
    "    plt.savefig(\"publications_by_country.png\", dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "create_choropleth_map(df_relevant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Create word clouds from abstracts and keywords\n",
    "# Combine all abstracts into a single string\n",
    "abstracts = \" \".join(df_relevant_data['abstract'].dropna())\n",
    "\n",
    "# Combine all keywords into a single string\n",
    "keywords = \" \".join([\" \".join(keyword_list) for keyword_list in keywords_list])\n",
    "\n",
    "# Generate and plot the word cloud for abstracts\n",
    "wordcloud_abstracts = WordCloud(width=800, height=400, background_color='white').generate(abstracts)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_abstracts, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud for Abstracts\")\n",
    "plt.savefig(\"wordcloud_abstracts.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# Generate and plot the word cloud for keywords\n",
    "wordcloud_keywords = WordCloud(width=800, height=400, background_color='white').generate(keywords)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_keywords, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud for Keywords\")\n",
    "plt.savefig(\"wordcloud_keywords.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf4bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
